# DistributedTraining
Implementing a method of training across multiple remote devices (with GPUs). The Naive_Parallel will not feature any pipelining, This will then be followed by a new implementation of the
Parallel training which will be similar to a stipped down version of PipeDream (https://arxiv.org/abs/1806.03377). 
